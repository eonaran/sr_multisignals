\documentclass[11pt]{article}

% Packages
\usepackage{amscd}
\usepackage{amsmath}
\usepackage{amstext}
\usepackage{bbold}
\usepackage{bm}
\usepackage{booktabs}
\usepackage{color}
\usepackage{easybmat}
\usepackage{etex}
\usepackage{framed}
\usepackage[dvips,letterpaper,margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage[noabbrev,capitalize]{cleveref}
\usepackage{mathtools}
\usepackage{setspace}
\usepackage{verbatim}
\usepackage[capitalize]{cleveref}
\usepackage{autonum}

%
% Commands
%

% Figures
\newcommand{\fig}[1]{(figure \ref{#1})}
\newcommand{\FIG}[1]{figure \ref{#1}}

\DeclareSymbolFont{bbold}{U}{bbold}{m}{n}
\DeclareSymbolFontAlphabet{\mathbbold}{bbold}

% Names
\newcommand{\Mobius}{M\"{o}bius}
\newcommand{\Holder}{H\"{o}lder}
\newcommand{\Rouche}{Rouch\'{e}}
\newcommand{\Ito}{It\={o}}
\newcommand{\Kondo}{Kond\^{o}}
\newcommand{\Levy}{L\'{e}vy}
\newcommand{\Cramer}{Cram\'{e}r}
\newcommand{\Godel}{G\"{o}del}
\newcommand{\Carath}{Carath\'{e}odory}
\newcommand{\Caratheodory}{Carath\'{e}odory}
\newcommand{\Hopital}{H\^{o}pital}

% Random
\renewcommand{\bar}{\overline}
\newcommand{\lvec}{\overrightarrow}
\newcommand{\ra}{\rangle}
\newcommand{\la}{\langle}

% Disjoint union
\makeatletter
\def\moverlay{\mathpalette\mov@rlay}
\def\mov@rlay#1#2{\leavevmode\vtop{%
   \baselineskip\z@skip \lineskiplimit-\maxdimen
   \ialign{\hfil$\m@th#1##$\hfil\cr#2\crcr}}}
\newcommand{\charfusion}[3][\mathord]{
    #1{\ifx#1\mathop\vphantom{#2}\fi
        \mathpalette\mov@rlay{#2\cr#3}
      }
    \ifx#1\mathop\expandafter\displaylimits\fi}
\makeatother

\newcommand{\cupdot}{\charfusion[\mathbin]{\cup}{\cdot}}
\newcommand{\bigcupdot}{\charfusion[\mathop]{\bigcup}{\cdot}}

% Blackboard bold
\newcommand{\RR}{\mathbb{R}}
\newcommand{\QQ}{\mathbb{Q}}
\newcommand{\NN}{\mathbb{N}}
\newcommand{\TT}{\mathbb{T}}
\newcommand{\ZZ}{\mathbb{Z}}
\newcommand{\DD}{\mathbb{D}}
\newcommand{\HH}{\mathbb{H}}
\newcommand{\CC}{\mathbb{C}}
\newcommand{\PP}{\mathbb{P}}
\newcommand{\EE}{\mathbb{E}}
\renewcommand{\AA}{\mathbb{A}}
\newcommand{\FF}{\mathbb{F}}
\renewcommand{\SS}{\mathbb{S}}
\newcommand{\Fp}{\FF_p}
\newcommand{\TrivGp}{\mathbbold{1}}
\newcommand{\One}{\mathbbold{1}}

\newcommand{\RP}{\RR\mathrm{P}}
\newcommand{\CP}{\CC\mathrm{P}}

% Vector bold
\newcommand{\nn}{\bm{n}}
\newcommand{\vv}{\bm{v}}
\newcommand{\ww}{\bm{w}}
\newcommand{\xx}{\bm{x}}
\newcommand{\yy}{\bm{y}}
\renewcommand{\vec}[1]{\mathbf{#1}}
\newcommand{\one}{\bm{1}}

% Other fonts
\newcommand{\fp}{\mathfrak{p}}
\newcommand{\fq}{\mathfrak{q}}
\newcommand{\fg}{\mathfrak{g}}
\newcommand{\fh}{\mathfrak{h}}
\newcommand{\fa}{\mathfrak{a}}
\newcommand{\fb}{\mathfrak{b}}
\newcommand{\fc}{\mathfrak{c}}
\newcommand{\fm}{\mathfrak{m}}
\renewcommand{\sl}{\mathfrak{sl}}
\newcommand{\so}{\mathfrak{so}}
\newcommand{\gl}{\mathfrak{gl}}
\renewcommand{\sp}{\mathfrak{sp}}
\newcommand{\sA}{\mathcal{A}}
\newcommand{\sB}{\mathcal{B}}
\newcommand{\sC}{\mathcal{C}}
\newcommand{\sD}{\mathcal{D}}
\newcommand{\sE}{\mathcal{E}}
\newcommand{\sF}{\mathcal{F}}
\newcommand{\sG}{\mathcal{G}}
\newcommand{\sH}{\mathcal{H}}
\newcommand{\sI}{\mathcal{I}}
\newcommand{\sL}{\mathcal{L}}
\newcommand{\sM}{\mathcal{M}}
\newcommand{\sN}{\mathcal{N}}
\newcommand{\sO}{\mathcal{O}}
\newcommand{\sP}{\mathcal{P}}
\newcommand{\sR}{\mathcal{R}}
\newcommand{\sS}{\mathcal{S}}
\newcommand{\sT}{\mathcal{T}}
\newcommand{\sU}{\mathcal{U}}
\newcommand{\sV}{\mathcal{V}}
\newcommand{\sX}{\mathcal{X}}
\newcommand{\sY}{\mathcal{Y}}

% Spacing
\newcommand\ThmBr{%
    \@ifstar{\item[\vbox{\null}]}{%
      \begingroup % keep changes local
      \setlength\itemsep{0pt}%
      \setlength\parsep{0pt}%
       \item[\vbox{\null}]%
      \endgroup%
     }}
\newcommand{\br}{\vspace{1pc}}
\newcommand{\BR}{\vspace{2pc}}
\newcommand{\picspace}{\vspace{13pc}}
\newcommand{\hs}{\hspace{1mm}}
\newcommand{\HS}{\hspace{3.5mm}}
\newcommand{\hr}{
  \begin{center}
    \line(1,0){250}
  \end{center}
}
\newcommand{\hrs}{
  \begin{center}
    \line(1,0){150}
  \end{center}
}

% Plain text
\renewcommand{\Re}{\mathrm{Re}}
\renewcommand{\Im}{\mathrm{Im}}

\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}

\newcommand{\disc}{\mathrm{disc}}
\newcommand{\Ann}{\mathrm{Ann}}
\newcommand{\Ass}{\mathrm{Ass}}
\newcommand{\Soc}{\mathrm{Soc}}
\newcommand{\Supp}{\mathrm{Supp}}
\newcommand{\Spec}{\mathrm{Spec}}
\newcommand{\maxSpec}{\mathrm{maxSpec}}

\newcommand{\N}{\mathrm{N}}
\newcommand{\Tr}{\mathrm{Tr}}

% Functors
\newcommand{\Hom}{\mathrm{Hom}}
\newcommand{\Der}{\mathrm{Der}}
\newcommand{\End}{\mathrm{End}}
\newcommand{\Ind}{\mathrm{Ind}}
\newcommand{\Aut}{\mathrm{Aut}}
\newcommand{\Gal}{\mathrm{Gal}}
\newcommand{\Sym}{\mathrm{Sym}}
\newcommand{\Rad}{\mathrm{Rad}}
\newcommand{\Id}{\mathrm{Id}}
\newcommand{\Ad}{\mathrm{Ad}}
\newcommand{\ad}{\mathrm{ad}}
\newcommand{\Pow}{\mathrm{Pow}}
\newcommand{\diam}{\mathrm{diam}}

\newcommand{\img}{\mathrm{img}}
\newcommand{\sgn}{\mathrm{sgn}}

\newcommand{\ch}{\mathrm{char}}
\newcommand{\Res}{\mathrm{Res}}
\newcommand{\ord}{\mathrm{ord}}
\newcommand{\cont}{\mathrm{cont}}
\newcommand{\ab}{\mathrm{ab}}
\newcommand{\Orb}{\mathrm{Orb}}
\newcommand{\Syl}{\mathrm{Syl}}
\newcommand{\Irr}{\mathrm{Irr}}
\newcommand{\Frac}{\mathrm{Frac}}
\newcommand{\sep}{\mathrm{sep}}
\newcommand{\per}{\mathrm{per}}

% Groups
\newcommand{\GL}{\mathrm{GL}}
\newcommand{\PGL}{\mathrm{PGL}}
\newcommand{\PSL}{\mathrm{PSL}}
\newcommand{\SL}{\mathrm{SL}}
\newcommand{\oO}{\mathrm{O}}
\newcommand{\SO}{\mathrm{SO}}
\newcommand{\PSO}{\mathrm{PSO}}
\newcommand{\Sp}{\mathrm{Sp}}
\newcommand{\PSp}{\mathrm{PSp}}
\newcommand{\U}{\mathrm{U}}
\newcommand{\SU}{\mathrm{SU}}
\newcommand{\PSU}{\mathrm{PSU}}

% Parentheses
\newcommand{\lgndr}[2]{\ensuremath{\left(\frac{#1}{#2}\right)}}

% Mappings
\newcommand{\iso}{\cong}
\newcommand{\eqdf}{\stackrel{\mathrm{df}}{=}}
\newcommand{\eqd}{\stackrel{\mathrm{d}}{=}}
\newcommand{\eqqu}{\stackrel{\mathrm{?}}{=}}
\newcommand{\xto}{\xrightarrow}
\newcommand{\dto}{\Rightarrow}
\newcommand{\into}{\hookrightarrow}
\newcommand{\xinto}{\xhookrightarrow}
\newcommand{\onto}{\twoheadrightarrow}
\newcommand{\xonto}{xtwoheadrightarrow}
\newcommand{\isoto}{\xto{\sim}}
\newcommand{\upto}{\nearrow}
\newcommand{\downto}{\searrow}

% Convenience
\newcommand{\Implies}{\ensuremath{\Rightarrow}}
\newcommand{\ImpliedBy}{\ensuremath{\Leftarrow}}
\newcommand{\Iff}{\ensuremath{\Leftrightarrow}}

\newcommand{\Pfright}{\ensuremath{(\Rightarrow):\hs}}
\newcommand{\Pfleft}{\ensuremath{(\Leftarrow):\hs}}

\newcommand{\sm}{\ensuremath{\setminus}}

\newcommand{\tab}[1]{(table \ref{#1})}
\newcommand{\TAB}[1]{table \ref{#1}}

\newcommand{\precode}[1]{\textbf{\footnotesize #1}}
\newcommand{\code}[1]{\texttt{\footnotesize #1}}

\newcommand{\sectionline}{
  \nointerlineskip \vspace{\baselineskip}
  \hspace{\fill}\rule{0.35\linewidth}{.7pt}\hspace{\fill}
  \par\nointerlineskip \vspace{\baselineskip}
}


%
% Misc
%

\parskip0em
\linespread{1.05}
\widowpenalty10000
\clubpenalty10000


\newcommand{\HC}{\mathsf{HC}}
\newcommand{\CHC}{\mathsf{CHC}}
\newcommand{\SK}{\mathsf{SK}}
\newcommand{\SDP}{\mathsf{SDP}}
\newcommand{\SOS}{\mathsf{SOS}}
\newcommand{\PE}{\mathsf{PE}}
\newcommand{\PS}{\mathsf{PS}}
\newcommand{\tEE}{\tilde{\mathbb{E}}}
\newcommand{\tCov}{\widetilde{\mathrm{Cov}}}
\newcommand{\sfP}{\mathsf{P}}
\newcommand{\argmin}{\mathrm{argmin}}
\newcommand{\argmax}{\mathrm{argmax}}
\newcommand{\diag}{\mathsf{diag}}
\newcommand{\vrad}{\mathrm{vrad}}
\newcommand{\rk}{\mathrm{rk}}
\newcommand{\rkeff}{\mathrm{rk}_{\mathrm{eff}}}
\newcommand{\GOE}{\mathsf{GOE}}
\newcommand{\ssG}{\mathsf{G}}
\newcommand{\balpha}{\bm \alpha}
\newcommand{\blambda}{\bm \lambda}
\newcommand{\bbeta}{\bm \beta}
\newcommand{\bA}{\bm A}
\newcommand{\bB}{\bm B}
\newcommand{\bD}{\bm D}
\newcommand{\bF}{\bm F}
\newcommand{\bG}{\bm G}
\newcommand{\bH}{\bm H}
\newcommand{\bP}{\bm P}
\newcommand{\bQ}{\bm Q}
\newcommand{\bR}{\bm R}
\newcommand{\bV}{\bm V}
\newcommand{\bW}{\bm W}
\newcommand{\bX}{\bm X}
\newcommand{\bY}{\bm Y}
\newcommand{\ba}{\bm a}
\newcommand{\bb}{\bm b}
\newcommand{\bc}{\bm c}
\newcommand{\bg}{\bm g}
\newcommand{\bv}{\bm v}
\newcommand{\bw}{\bm w}
\newcommand{\bx}{\bm x}

\newcommand{\bmat}[2]{
	\begin{bmatrix*}[#1]
		#2
	\end{bmatrix*}
}

\DeclareRobustCommand{\bmrob}[1]{\bm{#1}}
\pdfstringdefDisableCommands{%
  \renewcommand{\bmrob}[1]{#1}%
}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{question}{Question}
\newtheorem{definition}{Definition}
\newtheorem{example}{Example}
\newtheorem{proposition}{Proposition}
\newtheorem{corollary}{Corollary}
\newtheorem{conjecture}{Conjecture}

\title{Super-Resolution of Point Sources Down to the Rayleigh Limit from Multiple Observations}

\begin{document}

\maketitle

\noindent

\section{Introduction}

\subsection{Single Observation Problem Statement}

Suppose we observe a true signal whose representation in the physical domain is
\begin{equation}
    x^*(t) = \sum_{j = 1}^s a_j \delta_{t_j}(t),
\end{equation}
for $t \in \TT$ where $\TT = \RR / \ZZ$, which we will usually think of as the unit interval $[0, 1]$ with its endpoints identified, and $T = \{t_j\}_{j = 1}^s \subset \TT$ a discrete support set.
In the Fourier domain, this signal takes the form
\begin{equation}
    \widehat{x^*}(k) = \sum_{j = 1}^s a_j \exp(-2\pi i k t_j)
\end{equation}
for $k \in \ZZ$.

Now, we are interested in recovering this true signal $x$ from an observation that suffers from low resolution, which we represent as convolution of $x(t)$ with a point-spread function (PSF) denoted by $\phi(t)$.
We then observe the signal
\begin{equation}
    (\phi * x^*)(t) = \sum_{j = 1}^sa_j \phi(t - t_j).
\end{equation}
In the simplest case, $\phi$ is a Dirichlet kernel with cutoff frequency $f_c$, in which case the Fourier transform of the above is simply the truncation of the Fourier transform of $x^*$.
Denoting by $y$ the signal we observe in the Fourier domain, we have
\begin{equation}
    y(k) = \left(\sum_{j = 1}^s a_j \exp(-2\pi i k t_j)\right)\One\{|k| \leq f_c \}.
\end{equation}
We may then think of the data we observe as simply the $2f_c + 1$ values $y(-f_c), \dots, y(f_c)$.
We write $\sF_n$ for the sensing operator mapping $x^*$ to these $n$ Fourier coefficients.
A popular technique for solving this problem is \emph{total variation minimization}, which attempts to recover $x^*$ by solving the convex problem
\begin{equation}
    \argmin_x \|x\|_{\mathsf{TV}} \text{ subject to } \sF_nx = y.
\end{equation}

\subsection{Extension to Multiple Observations}

We now consider a generalization of the problem presented in the previous section, where we make \emph{several} observations signals sharing the support $T = \{t_1, \dots, t_s\}$ of $x^*$, but having varying amplitudes $a_j$.
To formalize this, our true signal is now
\begin{equation}
    x^*_\ell(t) = \sum_{j = 1}^s a_{\ell, j} \delta_{t_j} \text{ for } \ell \in \{1, \dots, m\},
\end{equation}
and our observations are $y_{\ell, k} = (\sF_n x^*_\ell)_{k}$ for $k \in \{-f_c, \dots, f_c\}$ and $\ell \in \{1, \dots, m\}$.
We think of the $a_{\ell, j}$ as organized into vectors $\ba_{j} \in \RR^m$ for $j \in \{1, \dots, s\}$, and the  $y_{\ell, k}$ as organized into a matrix $\bY \in \CC^{m \times n}$.
The \emph{group total variation minimization} is the natural extension of the previous convex problem to this setting, where we solve
\begin{equation}
    \argmin_x \|x\|_{\mathsf{gTV}} \text{ subject to } \left[\begin{array}{cccc}\sF_nx_1 & \sF_n x_2 \cdots \sF_n x_m \end{array}\right]^\top = \bY.
\end{equation}

\subsection{Dual Certificates}

In this section, we lay out the Lagrangian duality theory for TV and gTV norm minimization, the main tool for theoretical analysis of the performance of these algorithms.

\begin{definition}
    Let $\mu^0 \subset \CC$ denote the complex unit circle.
    A \emph{sign pattern} on a set is an assignment of points of $\mu^0$ to each point of the set.
\end{definition}

\begin{definition}
    For a sign pattern $v \in (\mu^0)^T$, a low-pass trigonometric polynomial $q: \TT \to \CC$,
    \begin{equation}
        q(t) = \sum_{k = -f_c}^{f_c} c_k e^{2\pi i k t},
    \end{equation}
    is a \emph{single-observation dual certificate} for $v$ if $q(t_j) = v_j$ for $t_j \in T$ and $|q(t)| < 1$ for $t \notin T$.
\end{definition}
\begin{proposition}[TV Norm Minimization Duality]
If a dual certificate for the sign pattern $v_j = a_j / |a_j|$ exists, then $x^*$ is the unique solution of TV minimization for the super-resolution problem.
\end{proposition}
Therefore, to prove the effectiveness of TV minimization it suffices to show that a dual certificate exists under some conditions on $T$.
The main result of \cite{fernandez2016super} establishes that this is true under a minimum separation condition.
\begin{theorem}[Proposition 2.3 of \cite{fernandez2016super}]
    If $\Delta(T) \geq 1.26\lambda_c$ and $f_c \geq 10^3$, then a dual certificate exists for any sign pattern on $T$.
    \label{thm:single-obs-recovery}
\end{theorem}

As also observed in \cite{fernandez2016super}, the same ideas extend to the multiple observation case in a straightforward fashion.
\begin{definition}
    Let $\mu^{m - 1} \subset \CC^m$ denote the $m$-dimensional complex unit sphere.
    A \emph{$m$-dimensional sign pattern} on a set is an assignment of points of $\mu^{m - 1}$ to each point of the set.
\end{definition}
\begin{definition}
    For a sign pattern $\bv \in (\mu^{m - 1})^T$, a low-pass trigonometric polynomial $q: \TT \to \CC^m$ given by $q(t) = (q_1(t), \dots, q_m(t))$ and
    \begin{equation}
        q_\ell(t) = \sum_{k = -f_c}^{f_c} c_k \exp(2\pi i k t) \text{ for } \ell \in \{1, \dots, m\}
    \end{equation}
    is an \emph{$m$-observation dual certificate} for $v$ if $q(t_j) = \bv_j$ for $t_j \in T$, and $\|q(t)\|_{\ell^2} < 1$ for $t \notin T$.
\end{definition}
\begin{proposition}[gTV Norm Minimization Duality]
    If a dual certificate for the sign pattern $\bv_j = \ba_{j} / \|\ba_{j}\|_{\ell^2}$ exists, then $x^*$ is the unique solution of gTV minimization for the super-resolution problem.
\end{proposition}
However, while numerical experiments in \cite{fernandez2016super} suggest that when the amplitude vectors $\ba_j$ are taken randomly then recovery is possible down to a lower critical minimum separation which accumulates at $\frac{1}{2}\lambda_c$ as $m \to \infty$, the same uniform argument cannot apply---indeed, it is always possible that the signs $\ba_j$ are all identical, in which case clearly it cannot be possible to recover $x^*$ any more effectively than in the one observation case.
Thus, to show that gTV norm minimization substantially improves on TV norm minimization, it is necessary to make some probabilistic assumptions about the data generating process of the $\ba_j$.

\section{Dual Certificate Construction}

Recall that we are interested in building a low-pass trigonometric polynomial $q: \TT \to \CC^m$ interpolating the points $(t_j, v_j)$ while remaining strictly inside the unit sphere in $\CC^m$ elsewhere.
The idea of the construction of \cite{fernandez2016super} when $m = 1$ is to build $q$ via kernel interpolation, and adjust it so that $q^\prime(t_j) = 0$ for each $j$, which one would hope might encourage the polynomial to remain in the unit sphere.
The naive extension of this to the $m$-dimensional is to require $\nabla q(t_j) = 0$ for each $t_j$.
However, numerical experiments show that the regime where this construction succeeds essentially does not depend on $m$, which is not surprising---after all, this is repeating the one-dimensional construction in each coordinate.
We propose a different dual certificate, which does succeed numerically in certifying reconstruction down to the Rayleigh limit as $m \to \infty$.

\subsection{Step 1: Relaxing Gradient Condition}

The first observation is that the condition $\nabla q(t_j) = 0$ is overkill for encouraging $q$ to remain in the unit sphere.
Intuitively, all that is really necessary is that $\nabla q(t_j)$ not have a component pointing out of the sphere, i.e.\ in the direction of $\bv_j$.
Thus, a reasonable condition is to require $\nabla q(t_j)$ and $\bv_j$, viewed as vectors, to be orthogonal.
However, it is not possible to express this constraint in complex arithmetic, so we must rewrite the dual certificate in terms of its real and complex parts (each of which is still low-pass).

Thus, we write $q_\ell(t) = q_\ell^R(t) + iq_\ell^I(t)$ for real-valued low-pass trigonometric polynomials $q_\ell^R$ and $q_\ell^I$.
We combine these into a single real-valued $2m$-dimensional trigonometric polynomial,
\begin{equation}
    \tilde{q}(t) = (q_1^R(t), q_1^I(t), \dots, q_m^R(t), q_m^I(t)) \in \RR^{2m}.
\end{equation}
Likewise writing $v_{j, \ell} = v_{j, \ell}^R + iv_{j, \ell}^I$ and $\tilde{\bv}_j = (v_{j, 1}^R, v_{j, 1}^I, \dots, v_{j, m}^R, v_{j, m}^I)$, we can express the interpolation conditions that the dual certificate $q$ must satisfy in terms of these new variables in essentially the same way: $\tilde{q}(t_j) = \tilde{\bv}_j$ for $j \in \{1, \dots, s\}$, and $\|\tilde{q}(t)\|_{\ell^2} < 1$ for all $t \notin T$.
We can now formulate the conditions we will require of our interpolant.
\begin{enumerate}
\item[I1.] $\tilde{q}(t_j) = \tilde{\bv}_j$ for $j \in \{1, \dots, s\}$.
\item[I2.] $\la \nabla \tilde{q}(t_j), \tilde{\bv}_j \ra = 0$ for $j \in \{1, \dots, s\}$.
\end{enumerate}

\subsection{Step 2: Kernel Interpolation}

We now proceed just as in \cite{fernandez2016super}: fix a real-valued low-pass kernel $K$, and define
\begin{align}
  q_\ell^R(t) &= \sum_{j = 1}^s \alpha_{j, \ell}^R K(t - t_j) + \sum_{j = 1}^s \beta_{j, \ell}^R K^\prime(t - t_j), \label{eq:interpolant-def-1} \\
  q_\ell^I(t) &= \sum_{j = 1}^s \alpha_{j, \ell}^I K(t - t_j) + \sum_{j = 1}^s \beta_{j, \ell}^I K^\prime(t - t_j), \label{eq:interpolant-def-2}
\end{align}
for coefficients $\alpha_{j, \ell}^R, \alpha_{j, \ell}^I, \beta_{j, \ell}^R, \beta_{j, \ell}^I \in \RR$ to be determined.
We think of these as organized into four matrices, $\balpha^R, \balpha^I, \bbeta^R, \bbeta^I \in \RR^{s \times m}$.
We also organize the real and imaginary parts of the sign patterns $v_{j, \ell}^R$ and $v_{j, \ell}^I$ into matrices $\bV^R, \bV^I \in \RR^{s \times m}$ of the same shape.
Conditions I1 and I2 from the previous part may now be expressed as a system of linear constraints on these coefficients, which we introduce some notation to write down concisely with some matrix algebra.
\begin{definition}
    For a function $K: \TT \to \RR$ and an ordered point set $T = \{t_1, \dots, t_s\}$, define $\bD(K, T) \in \RR^{s \times s}$ to have entries
    \begin{equation}
        (\bD(K, T))_{j, k} = K(t_j - t_k).
    \end{equation}
\end{definition}
The constraint I1 may then be written as the two linear matrix equations
\begin{align}
  \bD(K, T)\balpha^R + \bD(K^\prime, T)\bbeta^R &= \bV^R, \label{1045} \\
  \bD(K, T)\balpha^I + \bD(K^\prime, T)\bbeta^I &= \bV^I, \label{1046}
\end{align}
and the constraint I2 as the one equation
\begin{equation}
    \diag\bigg(\bD(K^{\prime}, T)\balpha^R\bV^{R^\top} + \bD(K^{\prime\prime}, T)\bbeta^R\bV^{R^\top} + \bD(K^{\prime}, T)\balpha^I\bV^{I^\top} + \bD(K^{\prime\prime}, T)\bbeta^I\bV^{I^\top}\bigg) = \bm 0. \label{1047}
\end{equation}
For numerical calculations, it is straightforward to unfold these constraints into a vectorized linear system for the coefficients.

\subsection{Step 3: Choosing Solution of Underconstrained System}

Note that the system described in the previous section has $4sm$ variables but only $2sm + s = (2m + 1)s$ constraints.
When the sign pattern is purely real, this reduces to $2s$ variables and $2s$ constraints, so there is (generically) a unique solution, as in the construction treated in \cite{fernandez2016super}.
In our more general setting, however, uniqueness typically fails, and so we must build a criterion for choosing a solution.

This extra freedom lets us further encourage $q(t)$ to remain inside the unit sphere.
In particular, a natural quantity to minimize is the $L^2$ norm of $q(t)$:
\begin{equation}
    \|q\|_{L^2(\TT)}^2 = \int_0^1 \|q(t)\|_{\ell^2}^2 dt.
\end{equation}
The added benefit of this choice is that it is a quadratic form in terms of the coefficients $\balpha, \bbeta$, and so its minimization subject to linear constraints is well-understood and has a convenient closed form.
We now develop some notation to write down this quadratic form.
\begin{definition}
    For real-valued functions $K_1, K_2 \in L^2(\TT)$ and an ordered point set $T = \{t_1, \dots, t_s\}$, define $\bG(K_1, K_2, T) \in \RR^{s \times s}$ to have entries
    \begin{equation}
        (\bG(K_1, K_2, T))_{j, k} = \int_0^1 K_1(t - t_j)K_2(t - t_k)dt.
    \end{equation}
  In the special case $K_1 = K_2 =: K$, we simply write $\bG(K, T)$ for the same matrix, and observe that it is the Gram matrix of the shifted functions $f_j(t) = K(t - t_j)$ in the Hilbert space $L^2(\TT)$.
\end{definition}
Now, the $L^2$ norm of the polynomial $q(t)$ with real and imaginary parts as defined in \eqref{eq:interpolant-def-1} and \eqref{eq:interpolant-def-2} respectively is given by
\begin{align}
  \|q\|_{L^2(\TT)}^2
  &= \sum_{X \in \{R, I\}}\sum_{\ell = 1}^m \sum_{j = 1}^s \sum_{k = 1}^s \bigg(\alpha_{\ell, j}^X \alpha_{\ell, k}^X \la K(\bullet - t_j), K(\bullet - t_k) \ra
  + \beta_{\ell, j}^X \beta_{\ell, k}^X \la K^\prime(\bullet - t_j), K^\prime(\bullet - t_k) \ra \nonumber \\
  &\hspace{2cm}+ \alpha_{\ell, j}^X \beta_{\ell, k}^X \la K^\prime(\bullet - t_j), K(\bullet - t_k) \ra\bigg) \\
  &= \sum_{X \in \{R, I\}} \bigg(\Tr(\balpha^{X^\top}\bG(K, T)\balpha^X) + \Tr(\bbeta^{X^\top}\bG(K^\prime, T)\bbeta^X) + \Tr(\bbeta^{X^\top}\bG(K^\prime, K, T)\balpha^X) \nonumber \\
  &\hspace{2cm}+ \Tr(\balpha^{X^\top}\bG(K, K^\prime, T)\bbeta^X)\bigg) \\
  &= \Tr\left( \left[\begin{array}{c} \balpha^R \\ \bbeta^R \\ \balpha^I \\ \bbeta^I \end{array}\right]^\top \left(\bm I_2 \otimes \left[
  \begin{array}{cc}
    \bG(K, T) & \bG(K, K^\prime, T) \\
    \bG(K^\prime, K, T) & \bG(K^\prime, T)
  \end{array}\right]\right)\left[\begin{array}{c} \balpha^R \\ \bbeta^R \\ \balpha^I \\ \bbeta^I \end{array}\right]\right), \label{1044}
\end{align}
where in the last expression each block of the inner matrix is a Gram matrix of the $2s$ functions $K(\bullet - t_j)$ and $K^\prime(\bullet - t_j)$ in $L^2(\TT)$.

\subsection{Numerical Evaluation}

\section{Proof}
We start by writing the Lagrangian of the optimization problem where we minimize \eqref{1044} with constraints I1 and I2 at \eqref{1045}, \eqref{1046} and \eqref{1047}.  

\begin{align}
&L(\balpha^{\{R,I\}},\bbeta^{\{R,I\}}, \blambda^{\{1,2,3\}}) = \sum_{X \in \{R, I\}} \bigg(\Tr(\balpha^{X^\top}\bG(K, T)\balpha^X) + \Tr(\bbeta^{X^\top}\bG(K^\prime, T)\bbeta^X) 
\\&+ \Tr(\bbeta^{X^\top}\bG(K^\prime, K, T)\balpha^X) + \Tr(\balpha^{X^\top}\bG(K, K^\prime, T)\bbeta^X) \bigg)\\
&-\Tr\left( (\blambda^1)^T\left( \bD(K, T)\balpha^R + \bD(K^\prime, T)\bbeta^R - \bV^R \right) \right) \\
& - \Tr\left( (\blambda^2)^T\left(\bD(K, T)\balpha^I + \bD(K^\prime, T)\bbeta^I - \bV^I\right) \right)\\
& - \Tr\bigg( (\blambda^3)^T\left( \bD(K^{\prime}, T)\balpha^R\bV^{R^\top} + \bD(K^{\prime\prime}, T)\bbeta^R\bV^{R^\top} + \bD(K^{\prime}, T)\balpha^I\bV^{I^\top} + \bD(K^{\prime\prime}, T)\bbeta^I\bV^{I^\top} \right)\bigg)  \label{1223}
\end{align}
where $ \blambda^1,\blambda^2\in\RR^{s\times m} $ and $ \blambda^3\in\RR^{s\times s} $  is a diagonal matrix. 

The first order optimality condition for $ L(\balpha^{\{R,I\}},\bbeta^{\{R,I\}}, \blambda^{\{1,2,3\}}) $ reads as
\begin{align}
	\nabla_{\balpha^R}L(\balpha^{\{R,I\}},\bbeta^{\{R,I\}}, \blambda^{\{1,2,3\}}) &= \bm 0 \quad\Leftrightarrow\\
	\left(G(K,T)+G(K,T)^T \right)\balpha^R + \left(G(K,K',T)^T+G(K',K,T)^T\right)\bbeta^R& - D(K,T)^T\blambda^1-D(K',T)^T\blambda^3\bm V^R = \bm 0\\ \\
\nabla_{\bbeta^R}L(\balpha^{\{R,I\}},\bbeta^{\{R,I\}}, \blambda^{\{1,2,3\}}) &= \bm 0 \quad\Leftrightarrow \\
\left(G(K',T)+G(K',T)^T \right)\bbeta^R + \left(G(K',K,T)^T+G(K,K',T)^T\right)\balpha^R &- D(K',T)^T\blambda^1-D(K'',T)^T\blambda^3\bm V^R = \bm 0\\\\
\nabla_{\balpha^I}L(\balpha^{\{R,I\}},\bbeta^{\{R,I\}}, \blambda^{\{1,2,3\}}) &= \bm 0 \quad\Leftrightarrow\\
\left(G(K,T)+G(K,T)^T \right)\balpha^I + \left(G(K,K',T)^T+G(K',K,T)^T\right)\bbeta^I &- D(K,T)^T\blambda^2-D(K',T)^T\blambda^3\bm V^I = \bm 0\\\\
\nabla_{\bbeta^I}L(\balpha^{\{R,I\}},\bbeta^{\{R,I\}}, \blambda^{\{1,2,3\}}) &= \bm 0 \quad\Leftrightarrow \\
\left(G(K',T)+G(K',T)^T \right)\bbeta^I + \left(G(K',K,T)^T+G(K,K',T)^T\right)\balpha^I &- D(K',T)^T\blambda^2-D(K'',T)^T\blambda^3\bm V^I = \bm 0
\end{align}
Define matrix $ \tilde{G}\in \RR^{2s\times 2s} $ as
\begin{align}
	\tilde{G}& = \bmat{c}{G(K,T)+G(K,T)^T & G(K,K',T) + G(K',K,T)^T \\ G(K,K',T)^T + G(K',K,T) &G(K',T)+G(K',T)^T }\\
	& =  2 \bmat{c}{G(K,T) & G(K,K',T)  \\ G(K,K',T)^T  &G(K',T) }
\end{align}
and $ \tilde{D}\in \RR^{2s\times 2s} $ as
\[\tilde{D} = \bmat{c}{D(K,T) & D(K',T)\\D(K',T) & D(K'',T)  }\]
Note $ \tilde{D} $ is a symmetric matrix if our kernel is even. 
Then first order conditions in compact form is
\begin{align}
	\tilde{G} \bmat{c}{\balpha^R\\ \bbeta^R} = \tilde{D} \bmat{c}{\blambda^1\\ \blambda^3 \bm V^R} 
\end{align}
and 
\begin{align}
\tilde{G} \bmat{c}{\balpha^I\\ \bbeta^I} = \tilde{D} \bmat{c}{\blambda^2\\ \blambda^3 \bm V^I} 
\end{align}
\bibliographystyle{unsrt}
\bibliography{main}

\end{document}
%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
